\documentclass[paper]{geophysics}
%\documentclass[paper,revised]{geophysics}
\usepackage{listings}
% An example of defining macros
\newcommand{\rs}[1]{\mathstrut\mbox{\scriptsize\rm #1}}
\newcommand{\rr}[1]{\mbox{\rm #1}}

\begin{document}

\title{Fast equivalent layer technique for gravity data processing with Block-Toeplitz Toeplitz-Block matrix systems}

\renewcommand{\thefootnote}{\fnsymbol{footnote}} 

\ms{GEO-Example} % paper number

\address{
\footnotemark[1]Observat\'orio Nacional, \\
77 General Jos\'e Cristino Av., \\
Rio de Janeiro, RJ, 20921400 \\
}
\author{Diego Takahashi\footnotemark[1], Vanderlei C. Oliveira Jr.\footnotemark[1] and Val\'eria
	C. F. Barbosa\footnotemark[1]}

%\footer{Example}
\lefthead{Takahashi, Oliveria Jr. \& Barbosa}
\righthead{\emph{Geophysics}}

\maketitle

\begin{abstract}
We present a new approach of the fast equivalent layer technique for gravity data processing, modifying \cite{siqueira2017fast}'s work, with the potential to use very large datasets at low computational cost. Taking advantage of the properties related to the symmetric Block-Toeplitz Toeplitz-block (BTTB) and Block-Circulant Circulant-Block (BCCB) matrices, that raises when regular grids of observation points and equivalent sources are used with the equivalent layer, we developed an algorithm which greatly reduces the number of flops and memory RAM necessary to complete the process of parameter estimative using this technique. The algorithm is based on the struture of symmetric BTTB matrices, where all its elements are comprised by the first row and can be embedded into a symmetric BCCB matrix, that also only needs its first row to be completed reconstructed. From the first column, the eigenvalues of BCCB matrices can be calculated using the Fast Fourier Transform, which can be used to readily compute matrix-vector products. Using examples, we demonstrate that even small and medium sized grids benefits from this approach and larger the dataset, faster and more efficient this method becomes compared to the usual. Synthetic tests using the equivalent layer with the method presented in this work evaluate this approach, and demonstrate satisfatory results for different gravity processing, as upward and downward continuation. Tests with real data from Caraj\'as, Brazil, shows its applicabillity and potential for field processing.
\end{abstract}

\section{Introduction}

The equivalent layer is a well-known technique for processing potential-field data in applied geophysics since the 60's. It comes from potential theory as a mathematical solution of the Laplace's equation, in the region above the sources, by using the Dirichlet boundary condition \cite[]{kellogg1929}.
This theory states that any potential field produced by an arbitrary 3D physical property distribution can be exactly reproduced by a fictitious layer located at any depth and having a continuos 2D physical property  distribution. In pratical situations, the layer is approximated by a finite set of sources (e.g., point masses or dipoles) and their physical properties are estimated by solving a linear system to fit the observed potential field. These fictitious sources are called equivalent sources.

Many previous works have used the equivalent layer as a processing technique in potential methods. \cite{dampney1969equivalent} used  for gridding and vertical continuation. \cite{cordell1992, mendonca-silva1994} used for data interpolation and gridding. \cite{emilia1973,hansen-miyazaki1984,li-oldenburg2010} for upward continuation. \cite{silva1986,leao-silva1989,guspi-novara2009,oliveirajr-etal2013} to reduction to the pole of magnetic data. \cite{boggs-dransfield2004} for combination of multiple data sets. \cite{barnes-lumley2011} for gradiente data processing.

The classic equivalent layer formulation consists on the multiplication between the property distribution and the kernel of the function. For gravity data, this kernel is a harmonic funtion related to the inverse of the distance between the observation point and the equivalent source. When these observation points and equivalent sources are regularly spaced, a Toeplitz system arises. Toeplitz systems are well-known in many fields of science as mathematics - numerical partial and ordinary diferential equations \cite[]{lin2003strang} -, image processing \cite[]{chan1999cosine} and neural networks \cite[]{wray1994calculation}. \cite{chan2007introduction} and \cite{jin2003developments} give many examples of applications for Toeplitz systems.

In potential methods the Toeplitz system properties was used for downward continuation \cite[]{zhang2016bttb} and for 3-D gravity field inversion using a 2-D multilayer model \cite[]{zhang2015bttb}. In the particular case of gravity data, the kernel generates a linear system with a matrix known as symmetric Block-Toeplitz Toeplitz-Block (BTTB). 

Because of the importance and vast occurency of Toeplitz systems, many authors studied methods for solving them. Direct methods were conceived by \cite{levinson1946wiener} and by \cite{trench1964algorithm}. Currently the conjugate gradient is used in most cases. In \cite{grenander1984szeg}, Szeg\"{o} noticed that a circulant matrix can be diagonalized by taking the fast Fourier trasform of its first column, making it possible to calculate the matrix-vector product and solve the system with low computational cost \cite[]{strang1986introduction,olkin1986linear}. \cite{chan2007introduction} shows some preconditioners to embedd the Toeplitz and BTTB matrices into circulant matrices and Block-Circulant Circulant-Block (BCCB), respectively, solving the system applying the conjugate gradient method.

Although the use of the equivalent layer technique increased over the last decades, one of the its biggest problem stills its high computational cost for processing large data sets. \cite{siqueira2017fast} developed a computationally efficient scheme for processing gravity data. This scheme does not solve a linear system, instead uses an iterative process that corrects the property distribution of the layer by proportionally adding mass to the gravity residual. Although of its efficiency, at each iteration the forward problem must be solved to evaluate the convergence of the estimative. This processes accounts for most of the computational cost of this method.

We propose the use of BTTB and BCCB matrices properties to solve the forward problem of \cite{siqueira2017fast}'s method in a more efficient way, resulting in faster parameter estimation and the possibility to use very large datasets. We show how the system memory RAM usage can be drastically decreased by calculating only the first row of the BTTB matrix and embedding into a BCCB matrix. Using the Szeg\"{o} theorema  combined with \cite{strang1986introduction} the matrix-vector product can be accomplished with very low cost, reducing in some orders of magnitude the number of operations required to complete the process. We present synthetic tests to validate our proposal and real field data from Caraj\'as, Brazil to demonstrate its applicability.


\section*{Methodology}
\subsection{Equivalent layer theory for gravity data}
In applied geophysics, the observed gravity disturbance $\delta g(x, y, z)$ \citep{heiskanen1967physical} is commomly approximated as the vertical component $z$ of the gravitational attraction produced by gravity sources, as follows:

\begin{equation}
\delta g(x, y, z) = c_{g} \, G \, \int\int\limits_{v}\int \rho(x^{\prime}, y^{\prime}, z^{\prime}) \frac{(z^{\prime} - z) \quad dx^{\prime}dy^{\prime}dz^{\prime}}{[(x - x^{\prime})^{2} + (y - y^{\prime})^{2} + (z - z^{\prime})^{2}]^{\frac{3}{2}}}\: ,
\label{eq:gravity-disturbance}
\end{equation}
where $c_{g} = 10^5$ is a constant transforming from $m/s^2$ to $mGal$, $G$ is the Newton's gravitational constant $(m^3/kg \, s^2)$ and $\rho(x^{\prime}, y^{\prime}, z^{\prime})$ is the density at the point $(x^{\prime}, y^{\prime}, z^{\prime})$ inside the volume $V$ of the source. This integral is defined in a Cartesian coordinate system with axis $x$ pointing to north, $y$ pointing to east and $z$ pointing downward.

According to the classic upward continuation integral \citep{henderson1960comprehensive, henderson1970validity}, it is possible to compute the gravity disturbance $\delta g(x_{i}, y_{i}, z_{i})$, at a point $(x_i, y_i, z_i)$, from the gravity disturbance $\delta g(x, y, z_{0})$ at the constant plane $z_{0}$:

\begin{equation}
\delta g(x_i, y_i, z_i) = \frac{1}{2\pi}  \int\limits_{-\infty}^{+\infty} \int\limits_{-\infty}^{+\infty} \,
\frac{\delta g(x, y, z_0) (z_{0} - z_{i})}{\left[(x_{i} - x)^{2} +
	(y_{i} - y)^{2} +
	(z_{i} - z_{0})^{2} \right]^{\frac{3}{2}}} \, 
dx \, dy,
\label{eq:upward-continuation-integral}
\end{equation}
where, $z_0 > z_i$.
Equation \ref{eq:upward-continuation-integral} shows that the gravity disturbance $\delta g(x_{i}, y_{i}, z_{i})$ is a convolution between $\delta g(x, y, z_{0})$ and another harmonic function on the horizontal plane $z=z_0$.
Multiplying and dividing equation \ref{eq:upward-continuation-integral} by $G$ and discretizing the integral, we obtain:

\begin{equation}
\delta g(x_{i}, y_{i}, z_{i}) = \sum_{j=1}^{N} p_j a_{ij} \; ,
\label{eq:integral-sum}
\end{equation}
where $a_{ij}$ is given by:

\begin{equation}
a_{ij}= c_{g} \, G \, \frac{(z_{j} - z_{i})}{\left[(x_{i} - x_{j})^{2} +	(y_{i} - y_{j})^{2} +	(z_{i} - z_{j})^{2} \right]^{\frac{3}{2}}}
\label{eq:aij}
\end{equation}

\noindent and $p_j$ is the coefficient representing the physical property of the $j$-th equivalent source. In this case, the harmonic function $a_{ij}$ represents the vertical component of the gravitational attraction exerted, at the point $(x_{i}, y_{i}, z_{i})$, by a point mass located at the point $(x_{j}, y_{j}, z_{j})$, with mass $p_{j}$.
Equation \ref{eq:integral-sum} can be expressed in matrix form as:

\begin{equation}
\mathbf{d}(\mathbf{p}) = \mathbf{A} \mathbf{p} \: ,
\label{eq:predicted-data-vector}
\end{equation}
where $\mathbf{d}(\mathbf{p})$ is an $N \times 1$ vector, whose i-th element is the predicted gravity disturbance $\delta g(x_{i}, y_{i}, z_{i})$, $\mathbf{p}$ is an $N \times 1$ parameter vector,  whose j-th element is the coefficient $p_{j}$ representing the physical property of the $j$-th equivalent source and $\mathbf{A}$ is  an $N \times N$ sensibility matrix, where each element $a_{ij}$ is given by equation 4 and defines the field produced by  the j-th equivalent source at the point $(x_{i}, y_{i}, z_{i})$.
The solution of the parameters \textbf{p} can be solved by a linear inversion minimizing the function:

\begin{equation}
\Psi(\mathbf{p}) = \theta_g(\mathbf{p}) + \mu \, \theta_m(\mathbf{p}) \: ,
\label{eq:goal-function}
\end{equation}
where $\theta_g(\mathbf{p})$ is the misfit function given by:

\begin{equation}
\theta_g(\mathbf{p}) = ||\mathbf{d}^{o}-\mathbf{d(p)}||_2^2 \: ,
\label{eq:goal-function_d}
\end{equation}
which is the euclidian norm of the residual between the observed data $\mathbf{d}^{o}$ and the predicted data $\mathbf{d(p)}$.

The function $\theta_m(\mathbf{p})$ is the regularization, for example, the zeroth-order Tikhonov:

\begin{equation}
\theta_m(\mathbf{p}) = ||\mathbf{p}||_2^2 \: ,
\label{eq:tikhonov-function}
\end{equation}
which is the euclidian norm of the parameters $\mathbf{p}$. The variable $\mu$ is a real positive number regularizing the parameter.

Derivating equation \ref{eq:goal-function} in relation to $\mathbf{p}$ and making equal to zero, it is possible to estimate the parameters:

\begin{equation}
\mathbf{p}^{\ast} = \left( \mathbf{A}^{\top}\mathbf{A} + 
\mu \, \mathbf{I} \right)^{-1}
\mathbf{A}^{\top} \mathbf{d}^{o} \,.
\label{eq:p-ast-parameter-space}
\end{equation}

\subsection{Fast equivalent layer technique}
In \cite{siqueira2017fast} the authors developed an iterative least-squares method to estimate the mass distribution of the equivalent layer based on the excess of mass and the positive correlation between the observed gravity data and the equivalent sources. This scheme is proven to have a better computational efficiency than the classical equivalent layer approach with data sets of at least 500 observation points, even using a large number of iterations.
This work also proves that the excess of mass of a body is proportional to the surface integration of the gravity data. Considering each equivalent source directly beneath the observation points, a initial approximation of mass distibution is made:

\begin{equation}
\mathbf{p}^0 = \tilde{\mathbf{A}}^{-1} \mathbf{d}^{o} \: ,
\label{eq:initial_m}
\end{equation}
where $\mathbf{d}^o$ is the gravity data and $\tilde{\mathbf{A}}^{-1} = \Delta s/(2 \pi \, G \, c_g)$ with $\Delta s$ being an element of area located at the vertical coordinate $z_i$ and centered at the horizontal coordinates $(x_i,y_i), \, i = 1,..., N$. At each $k$-th iteration a mass correction vector $\mathbf{\Delta m^k}$ for every source is calculated by minimizing the function:

\begin{equation}
\phi(\mathbf{\Delta p^k}) = ||\mathbf{d}^{o} - \mathbf{A}\hat{\mathbf{p}}^k - \tilde{\mathbf{A}}\mathbf{\Delta p^k}||_2^2
\label{eq:goal-function_fast}
\end{equation}

\noindent and the mass distribution of the equivalent sources is updated as:

\begin{equation}
\mathbf{\Delta p^{k+1}} = \mathbf{p^{k}} + \mathbf{\Delta p^{k}} \: .
\label{eq:update_m}
\end{equation}

\lstset{language=Python}
\lstset{frame=lines}
\lstset{caption={A \textit{Python} algorithm for \cite{siqueira2017fast}'s fast equivalent layer.}}
\lstset{label={lst:code_direct}}
\lstset{basicstyle=\footnotesize}
\begin{lstlisting}
diagonal_A = G*(10**5)*2*pi/(dx*dy)
for i in range (itmax):
	res = (data - A.dot(p))
	delta_p = res/diagonal_A
	p += delta_p
\end{lstlisting}

Each iteration of this algorithm, the matrix-vector product $\tensor{A} \mathbf{p}$ must be calculated to get a new residual $\mathbf{d^0} - \tensor{A} \mathbf{p}$. While it is true that for a small number of observation points this represents no computational effort, for very large data sets it is costful and can be overwhelming in terms of memory RAM to maintain such operation.

\subsection{Structure of sensibility matrix A}
If a gravity disturbance  data $\delta g(x_i, y_i, z_i)$ (equation \ref{eq:integral-sum}) is placed on a regular grid at the constant vertical coordinate $z_{i} = z_{1}$, $i = 1, \dots, N$, and there is an equivalent source located directly below each point of this grid at a constant depth $z_{0}$, the elements $a_{ij}$ (equation \ref{eq:aij}) can be rewritten as follows:

\begin{equation}
a_{ij} = c_{g} G 
\frac{\Delta z}{\left[(r_{ij})^{2} + (\Delta z)^{2}\right]^{\frac{3}{2}}} \: ,
\label{eq:a_ij_r_ij}
\end{equation}
where $\Delta z = z_{0} - z_{1}$ and $r_{ij} = \sqrt{(x_{i} - x_{j})^{2} + (y_{i} - y_{j})^{2}}$ represents the  relative horizontal distance between the $i$-th observation point and the $j$-th equivalent source. It is worth noting that: (i) $r_{ij} = r_{ji}$ for any pair $ij$ and (ii) $r_{ij}$ computed for different pairs $ij$ may assume the same value. As a consequence of these two properties, the matrix $\mathbf{A}$ (equation \ref{eq:predicted-data-vector}) has the structure of a symmetric Toeplitz by blocks, where each block is also a symmetric Toeplitz matrix (BTTB) \cite[]{chan2007introduction, golub2013}. 
For example, consider a regular grid of $N_{x} \times N_{y}$ points, where $N_{x} = 4$ and $N_{y} = 3$, comprising a total number of $N = N_{x} \times N_{y} = 12$ points. In this case, matrix $\mathbf{A}$ is $N_{x} \times N_{x}$ grid of matrices given by:

\begin{equation}
\tensor{A} =
\pmatrix{
	\tensor{A_{0}} & \tensor{A_{1}} & \tensor{A_{2}} & \tensor{A_{3}} \cr
	\tensor{A_{1}} & \tensor{A_{0}} & \tensor{A_{1}} & \tensor{A_{2}} \cr
	\tensor{A_{2}} & \tensor{A_{1}} & \tensor{A_{0}} & \tensor{A_{1}} \cr
	\tensor{A_{3}} & \tensor{A_{2}} & \tensor{A_{1}} & \tensor{A_{0}}
},
\label{eq:toeplitz_block_T}
\end{equation}
where each block is a $N_{y} \times N_{y}$ matrix given by:

\begin{equation}
\tensor{A}_{l} =
\pmatrix{
	a_{0}^{\ell} & a_{1}^{\ell} & a_{2}^{\ell} \cr
	a_{1}^{\ell} & a_{0}^{\ell} & a_{1}^{\ell} \cr
	a_{2}^{\ell} & a_{1}^{\ell} & a_{0}^{\ell}
},
\label{eq:toeplitz_element}
\end{equation}
where the elements $a^{l}_{k}$, $l = 0, \dots, N_{x} - 1$, $k = 0, \dots, N_{y} - 1$ are computed with equation \ref{eq:a_ij_r_ij}. In this case, the complete matrix $\mathbf{A}$ can be obtained by computing only its first row or column.

\subsection{BCCB matrix-vector product}
As previous discussed in the  Fast equivalent layer technique, the matrix-vector product accounts for most of the computational cost (Listing \ref{lst:code_direct}). When large data sets are used, this operation can take some time and even be prohibited by memory RAM shortage. In order to lessen this problem we transform the BTTB matrix $\mathbf{A}$ into a Block-Circulating Circulating-Block (BCCB) matrix $\mathbf{C}$ and use its eigenvalues to carry the product of $\mathbf{A}$ and an arbitrary vector $\mathbf{p}$. This strategy has been successfully applied by Zhang and Wong (2015) and Zhang et al. (2016) for optmizing the computational cost of 3D gravity inversion and downward continuation of potential field, respectively. Here, we use this strategy for improving the computational efficiency of the Fast equivalent layer method proposed by Siqueira et al. (2017).

Following the example of our BTTB matrix $\mathbf{A}$ (equation \ref{eq:toeplitz_block_T}), its transformation into a BCCB matrix $\mathbf{C}$ is given by:

\begin{equation}
\tensor{C} =
\pmatrix{
	\tensor{C_{0}} & \tensor{C_{1}} & \tensor{C_{2}} & \tensor{C_{3}} & \tensor{0} & \tensor{C_{3}} & \tensor{C_{2}} & \tensor{C_{1}} \cr
	\tensor{C_{1}} & \tensor{C_{0}} & \tensor{C_{1}} & \tensor{C_{2}} & \tensor{C_{3}} & \tensor{0} & \tensor{C_{3}} & \tensor{C_{2}} \cr
	\tensor{C_{2}} & \tensor{C_{1}} & \tensor{C_{0}} & \tensor{C_{1}} & \tensor{C_{2}} & \tensor{C_{3}} & \tensor{0} & \tensor{C_{3}} \cr
	\tensor{C_{3}} & \tensor{C_{2}} & \tensor{C_{1}} & \tensor{C_{0}} & \tensor{C_{1}} & \tensor{C_{2}} & \tensor{C_{3}} & \tensor{0} \cr
	\tensor{0} & \tensor{C_{3}} & \tensor{C_{2}} & \tensor{C_{1}} & \tensor{C_{0}} & \tensor{C_{1}} & \tensor{C_{2}} & \tensor{C_{3}} \cr
	\tensor{C_{3}} & \tensor{0} & \tensor{C_{3}} & \tensor{C_{2}} & \tensor{C_{1}} & \tensor{C_{0}} & \tensor{C_{1}} & \tensor{C_{2}} \cr
	\tensor{C_{2}} & \tensor{C_{3}} & \tensor{0} & \tensor{C_{3}} & \tensor{C_{2}} & \tensor{C_{1}} & \tensor{C_{0}} & \tensor{C_{1}} \cr
	\tensor{C_{1}} & \tensor{C_{2}} & \tensor{C_{3}} & \tensor{0} & \tensor{C_{3}} & \tensor{C_{2}} & \tensor{C_{1}} & \tensor{C_{0}}
},
\label{eq:circulant_elements}
\end{equation}
where each block $\mathbf{C}_{l}$ is:

\begin{equation}
\tensor{C}_{l} =
\pmatrix{
	\tensor{A}_{l} & \times \cr
	\times & \tensor{A}_{l}
} =
\pmatrix{
	a_{0}^{\ell} & a_{1}^{\ell} & a_{2}^{\ell} & 0^{\ell} & a_{2}^{\ell} & a_{1}^{\ell} \cr
	a_{1}^{\ell} & a_{0}^{\ell} & a_{1}^{\ell} & a_{2}^{\ell} & 0^{\ell} & a_{2}^{\ell} \cr
	a_{2}^{\ell} & a_{1}^{\ell} & a_{0}^{\ell} & a_{1}^{\ell} & a_{2}^{\ell} & 0^{\ell} \cr
	0^{\ell} & a_{2}^{\ell} & a_{1}^{\ell} & a_{0}^{\ell} & a_{1}^{\ell} & a_{2}^{\ell} \cr
	a_{2}^{\ell} & 0^{\ell} & a_{2}^{\ell} & a_{0}^{\ell} & a_{0}^{\ell} & a_{1}^{\ell} \cr
	a_{1}^{\ell} & a_{2}^{\ell} & 0^{\ell} & a_{2}^{\ell} & a_{1}^{\ell} & a_{0}^{\ell}
}.
\label{eq:toeplitz_circulant_element}
\end{equation}

Matrix $\mathbf{C}$ is a $4N_x N_y \times 4N_x N_y$ Block-Circulant formed by Circulant-Blocks matrix. This matrix is formed by a grid of $2N_x \times 2N_x$ blocks, where each block is a $2N_y \times 2N_y$ matrix.

The product $\mathbf{d} = \mathbf{A}\mathbf{p}$ (equation \ref{eq:predicted-data-vector}), then becomes:

\begin{equation}
\mathbf{C} \mathbf{v} = \mathbf{q} \: ,
\label{eq:BCCB_vector_product}
\end{equation}
where $\mathbf{v}$ and $\mathbf{q}$ are $4N_x N_y \times 1$ vectors given by:

\begin{equation}
\mathbf{v} =
\pmatrix{
	\mathbf{v}_{0} \cr
	\mathbf{v}_{1} \cr
	\vdots \cr
	\mathbf{v}_{N_x- 1} \cr
	\mathbf{0}_{(2 N_x N_y)}
}
\end{equation}

\noindent and

\begin{equation}
\mathbf{q} =
\pmatrix{
	\mathbf{q}_{0} \cr
	\mathbf{q}_{1} \cr
	\vdots \cr
	\mathbf{q}_{N_x - 1} \cr
	\mathbf{0}_{(2 N_x N_y)}
} \: ,
\end{equation}
where $\mathbf{0}_{(2 N_x N_y)}$ is a $2 N_x N_y \times 1$ vetor of zeros, and $\mathbf{v}_{\ell}$ and $\mathbf{q}_{\ell}$, $\ell = 0, \dots, N_x-1$ are:

\begin{equation}
\mathbf{v}_{\ell} =
\pmatrix{
	\mathbf{p}_{\ell} \cr
	\mathbf{0}_{(N_y)}
}
\end{equation}

\noindent and

\begin{equation}
\mathbf{q}_{\ell} =
\pmatrix{
	\mathbf{d}_{\ell} \cr
	\mathbf{0}_{(N_y)}
} \: ,
\end{equation}
where $\mathbf{0}_{(N_y)}$ is a $N_y \times 1$ vetor of zeros. 

By using the Kronecker product properties, the auxiliary matrix-vetor product can be rewritten as follows:

\begin{equation}
\mathbf{F}_{(2 N_x)}^{\ast} \left[ \mathbf{L} \circ \left( \mathbf{F}_{(2N_x)} \mathbf{V} \, \mathbf{F}_{(2N_y)} \right) \right] \mathbf{F}_{(2N_y)}^{\ast} = \mathbf{Q} \: ,
\label{eq:fft_q}
\end{equation}
where ``$\circ$'' denotes the Hadamard product, $\mathbf{L}$ is a $2N_x \times 2N_y$ row-oriented matrix containing the eigenvalues of $\mathbf{C}_{(BCCB)}$, and $\mathbf{V}$ and $\mathbf{Q}$ are $2N_x \times 2N_y$ row-oriented matrices obtained from the vectors $\mathbf{v}$ and $\mathbf{q}$.

One of the properties of BCCB matrices is that its eigenvalues can be calculated by a 2D Discrete Fourier Transform \cite[]{chan2007introduction}. This lead to a fast calculation of $\mathbf{L}$ using a 2D Fast Fourier Transform, making the matrix-vector product a low computational cost process.

Note that in general, the first column of blocks forming a BCCB matrix $\mathbf{C}_{\ell}$ (equation \ref{eq:circulant_elements}) is given by:

\begin{equation}
\left[\mathbf{C} \right]_{(0)} = 
\pmatrix{
\mathbf{C}_{0} \cr
\mathbf{C}_{1} \cr
\vdots \cr
\mathbf{C}_{N_x-2} \cr
\mathbf{C}_{N_x-1} \cr
\mathbf{0} \cr
\mathbf{C}_{N_x-1} \cr
\mathbf{C}_{N_x-2} \cr
\vdots \cr
\mathbf{C}_{1}
} \; ,
\end{equation}
where each block $\mathbf{C}_{\ell}$, $\ell = 0, \dots, N_x-1$, is a $2N_y \times 2N_y$ circulant matrix and $\mathbf{0}$ is a $2N_y \times 2N_y$ matrix with all elements equal to zero. Thus, the first column of a circulant matrix $\mathbf{C}_{\ell}$ is given by:

\begin{equation}
\left[\mathbf{C}_{\ell} \right]_{(0)} =
\pmatrix{
	a^{\ell}_{00} \cr
	a^{\ell}_{10} \cr
	\vdots \cr
	a^{\ell}_{(N_y-2)0} \cr
	a^{\ell}_{N_y-1) 0} \cr
	0 \cr
	a^{\ell}_{N_y-1) 0} \cr
	a^{\ell}_{(N_y-2)0} \cr
	\vdots \cr
	a^{\ell}_{10}
} \:.
\end{equation}

To complete the process, after calculating the inverse of $\mathbf{Q}$ it is necessary to rearrange its rows to obtain the vector $\mathbf{q}$ and also rearrange the elements of $\mathbf{q}$ to obtain the wanted vector $\mathbf{d(p)}$.

%The algorithm for computing the matrix-vector product $\mathbf{A}\mathbf{p}$ and obtaining the resulting vector $\mathbf{d(p)}$ can be summarized in the following steps:

%1. Compute the first column of the embedded $4N_x N_y \times 4N_x N_y$ BCCB matrix $\mathbf{C}$ by using the first column of the $N_x N_y \times N_x N_y$ BTTB matrix $\mathbf{A}$;

%2. Compute the eigenvalues of $\mathbf{C}$ rearranging them in rows of the $2N_x \times 2N_y$ matrix $\mathbf{L}$;

%3. Rearrange the vector $\mathbf{v}$ in the rows of the $2N_x \times 2N_y$ matrix $\mathbf{V}$;

%4. Compute the Hadammard product $\mathbf{L} \circ \tilde{\mathbf{P}}$, where $\tilde{\mathbf{P}} = \mathbf{F}_{(2N_x)} \mathbf{P} \, \mathbf{F}_{(2N_y)}$ is computed by using a fast algorithm for 2D DFT;

%5. Compute the resulting $2N_x \times 2N_y$ matrix $\mathbf{Q} = \mathbf{F}_{(2N_x)}^{\ast} \left[ \mathbf{L} \circ \tilde{\mathbf{P}}\right] \mathbf{F}_{(2N_y)}^{\ast}$ by using a fast algorithm for inverse 2D DFT;

%6. Rearrange the rows of $\mathbf{Q}$ to obtain the vector $\mathbf{q}$;

%7. Rearrange the elements of $\mathbf{q}$ to obtain the vector $\mathbf{d(p)}$.

\subsection{Computational performance}
Equation \ref{eq:goal-function_fast} now can be calculated with the approach presented in the previous section. In a normal procedure of the fast equivalent layer, at each iteration a full matrix $\tensor{A}$ (equation \ref{eq:predicted-data-vector}) is multiplied by the estimated mass distribution parameter vector $\hat{\mathbf{p}}^k$. As pointed in \cite{siqueira2017fast} the number of flops (floating-point operations) necessary to estimate the $N$-dimensional parameter vector inside the iteration loop is:

\begin{equation}
f_0 = N^{it} (3N + 2N^2).
\label{eq:float_classic}
\end{equation}

From equation \ref{eq:float_classic} it is clear that the matrix-vector product ($2N^2$) accounts for most of the computational complexity in this method.

It is well known that FFT takes $N \log_2(N)$ flops \cite[]{brigham1988fast}. Computing the eigenvalues of the BCCB matrix ($4N \times 4N$) and applying 2D-FFT on the parameter vector (equation \ref{eq:fft_q}), takes $4N \log(4N)$ each. The point-multiplication takes $4N$. As it is necessary to compute the inverse FFT another two $4N \log(4N)$ must be taken in account. However, the sensibility matrix does not change during the process, thus, the eigenvalues of BCCB must be calculated only once, outside of the iteration. This lead us to:

\begin{equation}
f_1 = N^{it} (7N + 12N\log(4N)).
\label{eq:float_bccb}
\end{equation}

Another major improvement of this methodology is the exoneration of calculating the full sensibility matrix $\tensor{A}$ (equation \ref{eq:predicted-data-vector}). Each element needs $12$ flops (equation \ref{eq:aij}), totalizing $12N^2$ flops for the full matrix. Calculating only the first row of the BTTB matrix, $12N$ flops is required and the computation of the eigenvalues is $4N \log(4N)$ as mentioned above.
The full flops count of \cite{siqueira2017fast}'s method:

\begin{equation}
f_s = 12N^2 + N^{it} (3N + 2N^2),
\label{eq:float_siqueira}
\end{equation}

is decreased in our method to:

\begin{equation}
f_s = 12N + 4N \log(4N) + N^{it} (7N + 12N\log(4N)).
\label{eq:float_new}
\end{equation}

Figure \ref{fig:float} shows the floating points to estimate the parameter vector using the fast equivalent layer with \cite{siqueira2017fast}'s method (equation \ref{eq:float_classic}) and our approach (equation \ref{eq:float_bccb}) versus the number of observation points varyig from $N = 5000$ to $N = 1000000$ with $50$ iterations. The number of operations is drastically decreased.

Table 1 shows the system memory RAM usage needed to store the full matrix, the BTTB first row and the BCCB eigenvalues ($8$ times the BTTB first row). The quantities were computed for different numbers of data (N) with the same corresponding number of equivalent sources (N). This table considers that each element of the matrix is a double-precision number, which requires 8 bytes of storage, except for the BCCB complex eigenvalues, which requires 16 bytes per element. Notice that $1000000$ observation points requires nearly $7.6$ Terabytes of memory ram to store the whole sensibility matrix of the equivalent layer.

Using a PC with a Intel Core i7 4790@3.6GHz processor and 16 Gb of memory RAM, figure \ref{fig:time_comparison} shows the time necessary to run 50 iterations of the \cite{siqueira2017fast}'s method and the one presented in this work. After $10000$ observations points is clear how this method benefits from the new approach in calculating the forward problem. Because of the memory RAM available in this system, we could not test the comparison with more observations, limited to $22500$. In figure \ref{fig:time_bccb} we show the time necessary to run the equivalent layer technique with 50 iterations using only the new approach, where the RAM is not a limitation factor. We could run up to $25$ million observation points. In comparison, one million observation points took $26.8$ seconds to run, where the maximum $22500$ observation points, with \cite{siqueira2017fast}'s method, took $48.3$ seconds.

\section{Synthetic tests}
The synthetic data presented in this section has the objective to validate this new approach when used jointly with the fast equivalent layer presented in \cite{siqueira2017fast}. We constructed a model with two polygonal prisms, with density contrast of $0.35$ (upper-left body) and $0.4 g/cm^3$ (upper-right body), and a sphere with radius of $1000 m$ with density contrast of $-0.5 g/cm^3$. The vertical component of gravity generated by this bodies were calculated and are shown in figure \ref{fig:synthetic_data} together with their horizontal projections. A gaussian noise was added to the data, with mean of zero and a maximum of $0.5\%$ of the maximum value of the original data. As previous said, only in regular grids the BTTB matrix structures appears. We created $10000$ observation points regularly spaced in a grid of $100 \times 100$, with an uniform $100$ m of height for all the observations.

In figure \ref{fig:classic_fast} we show the fitted data with the fast equivalent layer using \cite{siqueira2017fast}'s work. In comparison we have the figure \ref{fig:bccb_fast} showing the fitted data with the new approach presented in this work by calculating the forward problem using equation \ref{eq:BCCB_vector_product}. In both figures A) is the original contaminated synthetic data, B) is the predict data by the equivalent layer and C) is the residual between the synthetic data and fitted data, with mean of $-8.264e^{-7}$ and standard deviation of $0.0144$. As we can see in the figure \ref{fig:delta_comparison}, there is virtually no difference in the fitted data presented in figures \ref{fig:classic_fast}b and \ref{fig:bccb_fast}c, showing that the result associated by calculating a matrix-vector product of a embedded BTTB into a BCCB matrix using equation \ref{eq:BCCB_vector_product} is the same as a normal matrix-vecotr product. In figure \ref{fig:delta_rho} we have the difference between the mass distribution of the equivalent sources estimated by using the two methods. 

In figure \ref{fig:upward_comparison} and figure \ref{fig:downward_comparison} we show two forms of processing a gravity data using the equivalent layer, the upward and the downward continuation, respectively. The upward height is $300 m$ and the downward is at $50 m$. Both figures show: A) the upward or downward in the traditional \cite{siqueira2017fast}'s work B) the upward or downward using this new approach and C) the residuals between the two forms of processing. For the upward continuation the mean of the residuals is $-5.938e^{-18}$ and the standard deviation is $8.701e^{-18}$. For the downward continuation the mean of the residuals is $5.914e^{-18}$ and the standard deviation is $9.014e^{-16}$. With \cite{siqueira2017fast}'s method the upward processing took $7.62026$ seconds and $0.00834$ seconds with the new approach. For the downward processing the times necessary were $7.59654$ seconds and $0.00547$ seconds, respectively.

\section{Real data tests}
Tests with real data are conducted with the gravity data of Caraj\'as provided by CPRM (Companhia de Pesquisa de Recursos Minerais). This area covers the southeast part of the state of Par\'a, Brazil. Aeromagnetic and aerogravimetric data were collected in $113$ flight lines with $3$ km apart from each other with N-S orientation. There was two separated teams for data collection, each responsible for a determined area. For gravity data the sample spacing were $7.65 m$ and $15.21 m$ for each team, totalizing more $4353428$ observation points. The height of the flights were fixed at $900 m$. All $4353428$ million gravity data were gridded into a regularly spaced dataset of $250000$ observation points ($500 \times 500$) for processing (figure \ref{fig:carajas_real_data}).

Figure \ref{fig:carajas_gz_predito} shows the gridded gravity data (A), the fitted data with 50 iterations of the fast equivalent layer at $300$ m depth using this new approach (B) and the residual (C). The mean of the residual was $0.000292$ and standard deviation of $0.105$ which demonstrates a good fit for the predicted data, evaluating this technique to be applied in real field data.

An upward continuation processing were made (Figure \ref{fig:up2000_carajas_500x500}) at $5000$ m over the real data. It shows a reasonable processing, atenuating the short-wave lenghts. The processing of the $250000$ observations points took $0.216$ seconds.

\section{Conclusions}
By exploring the properties related to Block-Toeplitz Toeplitz-block (BTTB) and Block-Circulant Circulant -Block (BCCB) matrices, we show a new approach for calculating the matrix-vector product of the fast equivalent layer technique from \cite{siqueira2017fast}'s work when regular grids of observation points and equivalent sources are employed. This alghorithm greatly reduces the number of flops necessary to complete the estimative of the equivalent layer. For example, when processing one million observation points, the number of flops is reduced in $10^4$ times. When processing such amount of data, the full sensibility matrix takes $7.6$ Terabytes of memory RAM storage, which is impractical, while takes only $61.035$ Megabytes with the method presented in this work.

This approach takes advantage of the symmetric BTTB system that arises when processing a harmonic function as the vertical component of gravity, that depends on the inverse of distance. Symmetric BTTB matrices can be stored by its only first row and can be embedded into a symmetric BCCB matrix that also only needs its first row. Using the Fast Fourier Transform it is possible to calculate the eigenvalues of BCCB matrices which can be used to compute a matrix-vector product in a very low computational cost. The time necessary to process medium sized grids of observational data, for example $22500$ points, is cutted in $10^2$ times.

Synthetic and real data tests were conducted with the equivalent layer using the method presented in this work, showing satisfatory results that evaluate it to process gravity data, as upward and downward continuations, even using very large datasets. In the future, applications of the equivalent layer using great amount of data, as in continental or global scale can be researched.
\newpage



\subsection*{Figures}
\renewcommand{\figdir}{Fig} % figure directory

Figure~\ref{fig:float}

\plot{float}{width=\textwidth}
{floating points to estimate the parameter vector using the fast equivalent layer with \cite{siqueira2017fast}'s method (equation \ref{eq:float_classic}) and our approach (equation \ref{eq:float_bccb}) versus the numbers of observation points varyig from $N = 5000$ to $N = 1000000$ with $50$ iterations. The number of operations is drastically decreased.}
\newpage

Figure~\ref{fig:time_comparison}

\plot{time_comparison}{width=10cm}
{time necessary to run 50 iterations of the \cite{siqueira2017fast}'s method and the one presented in this work. With the limitation of $16$ Gb of memory RAM in our system, we could test only up to $22500$ obervation points.}

Figure~\ref{fig:time_bccb}

\plot{time_bccb}{width=10cm}
{time necessary to run the equivalent layer technique with 50 iterations using only this new approach, where the RAM is not a limitation factor. We could run up to $25$ million observation points. In comparison, $1$ million observation points took $26.8$ seconds to run, where the maximun $22500$ observation points in figure \ref{fig:time_comparison}, with Siqueira's method, took $48.3$ seconds.}
\newpage

Figure~\ref{fig:synthetic_data}

\plot{synthetic_data}{width=13.5cm}
{model with two polygonal prisms, with density contrast of $0.35$ (upper-left body) and $0.4 g/cm^3$ (upper-right body), and a sphere with radius of $1000 m$ with density contrast of $-0.5 g/cm^3$. The vertical component of gravity generated by this bodies were calculated and are shown together with their horizontal projections. A gaussian noise was added to the data with mean of zero and maximum value of $0.5\%$ of the maximum of the original data. As previous said only in regular grids the BTTB matrix structures appears. We created $10000$ observation points regularly spaced in a grid of $100 \times 100$, with a uniform $100 m$ of height for all the observations.}
\newpage

Figure~\ref{fig:classic_fast}

\plot{classic_fast}{width=5cm}
{A) original contaminated synthetic data, B) fitted data using the fast equivalent layer with the \cite{siqueira2017fast}'s work. C) residual between the synthetic data and fitted data, with mean of $-8.264e^{-7}$ and standard deviation of $0.0144$.}
\newpage

Figure~\ref{fig:bccb_fast}

\plot{bccb_fast}{width=5cm}
{A) original contaminated synthetic data, B) fitted data with the new approach presented in this work of calculating the forward problem using equation \ref{eq:BCCB_vector_product} for the equivalent layer. C) residual between the synthetic data and fitted data, with mean of $-8.264e^{-7}$ and standard deviation of $0.0144$.}
\newpage

Figure~\ref{fig:delta_comparison}

\plot{delta_comparison}{width=13.5cm}
{virtually zero difference in the fitted data presented in figures \ref{fig:classic_fast}b and \ref{fig:bccb_fast}c, showing that matrix-vector product of a embedded BTTB into a BCCB matrix using equation \ref{eq:BCCB_vector_product} is pratically equal to a normal matrix-vector product.}
\newpage

Figure~\ref{fig:delta_rho}

\plot{delta_rho}{width=13.5cm}
{small difference in the estimated density between the classic matrix-vector product and the matrix-vector product of a embedded BTTB into a BCCB matrix using equation \ref{eq:BCCB_vector_product}, showing that the estimative using this work's method is pratically the same.}
\newpage

Figure~\ref{fig:upward_comparison}

\plot{upward_comparison}{width=5cm}
{A) the upward continuation conducted at the height of $300 m$ using the traditional \cite{siqueira2017fast}'s work B) the upward continuation conducted at the same height of $300 m$ using this new approach and C) residuals between the two forms of processing with mean of $-5.938e^{-18}$ and standard deviation of $8.701e^{-18}$. With \cite{siqueira2017fast}'s method this processing took $7.62026$ seconds and $0.00834$ seconds with the new approach.}
\newpage

Figure~\ref{fig:downward_comparison}

\plot{downward_comparison}{width=5cm}
{A) the downward continuation conducted at the height of $50 m$ using the traditional \cite{siqueira2017fast}'s work B) the upward continuation conducted at the same height of $50 m$ using this new approach and C) residuals between the two forms of processing with mean of $5.914e^{-18}$ and standard deviation of $9.014e^{-16}$. With \cite{siqueira2017fast}'s method this processing took $7.59654$ seconds and $0.00547$ seconds with the new approach.}
\newpage

Figure~\ref{fig:carajas_real_data}

\plot{carajas_real_data}{width=13.5cm}
{real data gravity data of Caraj\'as gridded into a regularly spaced dataset of $250000$ observation points ($500 \times 500$). This area covers the southeast part of the state of Par\'a, Brazil. Aerogravimetric data was collected in $113$ flight lines with $3 km$ apart from each other and N-S orientation, totalizing more than 4 million observation points. The height of the flights were fixed at $900 m$. All $4 million$ gravity data were gridded for processing.}
\newpage

Figure~\ref{fig:carajas_gz_predito}

\plot{carajas_gz_predito}{width=5cm}
{A) gridded gravity data. B) fitted data with 50 iterations of the fast equivalent layer at $400$ m depth using the new approach of this work. C) residual, defined as the difference between A) and B). The mean of the residual was $0.000292$ and standard deviation of $0.105$ which demonstrates a good fit for the predicted data, evaluating this technique to be applied in real field data.}
\newpage

Figure~\ref{fig:up2000_carajas_500x500}

\plot{up2000_carajas_500x500}{width=13.5cm}
{upward continuation processing at $2000 m$ over the real data of Caraj\'as. It shows a reasonable processing, atenuating the short-wave lenghts. The processing of the $250000$ observations points took $0.216$ seconds.}
\newpage

%\subsubsection{Multiplot} 
%Sometimes it is convenient to put two or more figures from different
%files in an array (see Figure~\ref{fig:exph,exgr}). Individual plots
%are Figures~\ref{fig:exph} and~\ref{fig:exgr}.
%
%\multiplot{2}{exph,exgr}{width=0.4\textwidth}
%{This figure is specified in the document by \newline \texttt{
%    $\backslash$multiplot\{2\}\{exph,exgr\}\{width=0.4$\backslash$textwidth\}\{This caption.\}}.
%}
%
%The first argument of the \texttt{multiplot} command specifies the
%number of plots per row.

\subsection{Tables}

\begin{table}[h]
	\begin{center}
		\begin{tabular}{|l|c|c|c|}
			\hline
			\textbf{$N \times N$} & \textbf{Full RAM (Mb)} & \textbf{BTTB RAM (Mb)}  & \textbf{BCCB RAM (Mb)}\\
			\hline 
			$100 \times 100$ & 0.0763 & 0.0000763 & 0.0006104\\
			\hline
			$400 \times 400$ & 1.22 & 0.0031 & 0.0248\\
			\hline
			$2500 \times 2500$ & 48 & 0.0191 & 0.1528\\
			\hline
			$10000 \times 10000$ & 763 & 0.00763 & 0.6104\\
			\hline
			$40000 \times 40000$ & 12207 & 0.305 & 2.4416 \\
			\hline
			$250000 \times 250000$ & 476837 & 1.907 & 15.3 \\
			\hline
			$500000 \times 500000$ & 1907349 & 3.815 & 30.518 \\
			\hline
			$1000000 \times 1000000$ & 7629395 & 7.629 & 61.035 \\
			\hline
		\end{tabular}
		\caption{Comparison between the system memory RAM usage needed to store the full matrix, the BTTB first row and the BCCB eigenvalues (eight times the BTTB). The quantities were computed for different numbers of data (N) with the same corresponding number of equivalent sources (N). This table considers that each element of the matrix is a double-precision number, which requires 8 bytes of storage, except for the BCCB complex eigenvalues, which requires 16 bytes per element.}
	\end{center}
\end{table} 

%The discussion is summarized in Table~\ref{tbl:example}.
%
%\tabl{example}{This table is specified in the document by \texttt{
%    $\backslash$tabl\{example\}\{This caption.\}\{\ldots\}}.
%}{
%  \begin{center}
%    \begin{tabular}{|c|c|c|}
%      \hline
%      \multicolumn{3}{|c|}{Table Example} \\
%      \hline
%      migration\rule[-2ex]{0ex}{5ex} & 
%      $\omega \rightarrow k_z$ & 
%      $k_y^2+k-z^2\cos^2 \psi=4\omega^2/v^2$ \\
%      \hline
%      \parbox{1in}{zero-offset\\diffraction}\rule[-4ex]{0ex}{8ex} &
%      $k_z\rightarrow\omega_0$ &
%      $k_y^2+k_z^2=4\omega_0^2/v^2$ \\
%      \hline
%      DMO+NMO\rule[-2ex]{0in}{5ex} & $\omega\rightarrow\omega_0$ & 
%      $\frac{1}{4}
%      v^2k_y^2\sin^2\psi+\omega_0^2\cos^2\psi=\omega^2$ \\
%      \hline
%      radial DMO\rule[-2ex]{0in}{5ex} & $\omega\rightarrow\omega_s$ &
%      $\frac{1}{4}v^2k_y^2\sin^2\psi+\omega_s^2=\omega^2$\\
%      \hline
%      radial NMO\rule[-2ex]{0in}{5ex} & $\omega_s\rightarrow\omega_0$ &
%      $\omega_0\cos\psi=\omega_s$\\
%      \hline
%    \end{tabular}
%  \end{center}
%}

\section{ACKNOWLEDGMENTS}

This study was financed by the brazilian agencies CAPES (in the form of a scholarship), FAPERJ (grant n.$^{\circ}$ E-26 202.729/2018) and CNPq (grant n.$^{\circ}$ 308945/2017-4).

%\append{Appendix example}

%\append[equations]{Another appendix}

%\sideplot{errgrp}{width=0.8\textwidth}
%{This figure is specified in the document by \texttt{
%    $\backslash$sideplot\{errgrp\}\{width=0.8$\backslash$text\-width\}\{This caption.\}}.


%\append{The source of this document}

%\verbatiminput{geophysics_example.ltx}

%\append{The source of the bibliography}

%\verbatiminput{example.bib}

\newpage

\bibliographystyle{seg}  % style file is seg.bst
\bibliography{references}

\end{document}
